import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
import pickle

# T√©l√©charger les stopwords (√† faire une seule fois)
nltk.download('stopwords')

class TextPreprocessor:
    def __init__(self):
        self.stop_words = set(stopwords.words('english'))
        self.vectorizer = TfidfVectorizer(max_features=3000)
    
    def clean_text(self, text):
        """Nettoie un texte"""
        # Mettre en minuscules
        text = text.lower()
        
        # Supprimer les caract√®res sp√©ciaux et chiffres
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        
        # Supprimer les espaces multiples
        text = re.sub(r'\s+', ' ', text).strip()
        
        # Supprimer les stopwords
        words = text.split()
        words = [word for word in words if word not in self.stop_words]
        
        return ' '.join(words)
    
    def prepare_data(self, df):
        """Pr√©pare le dataset complet"""
        # Nettoyer tous les messages
        print("üîÑ Nettoyage des textes...")
        df['cleaned_message'] = df['message'].apply(self.clean_text)
        
        # Convertir les labels en 0 et 1
        df['label_num'] = df['label'].map({'ham': 0, 'spam': 1})
        
        return df
    
    def vectorize(self, messages, fit=True):
        """Convertit les textes en vecteurs TF-IDF"""
        if fit:
            return self.vectorizer.fit_transform(messages)
        else:
            return self.vectorizer.transform(messages)
    
    def save_vectorizer(self, filename='models/vectorizer.pkl'):
        """Sauvegarde le vectorizer"""
        with open(filename, 'wb') as f:
            pickle.dump(self.vectorizer, f)
        print(f"‚úÖ Vectorizer sauvegard√© : {filename}")


def main():
    # Charger le dataset
    print("üìÇ Chargement du dataset...")
    df = pd.read_csv('data/spam.csv', encoding='latin-1')
    df = df[['v1', 'v2']]
    df.columns = ['label', 'message']
    
    # Pr√©traitement
    preprocessor = TextPreprocessor()
    df = preprocessor.prepare_data(df)
    
    # Afficher des exemples
    print("\n=== Exemple de nettoyage ===")
    print("AVANT:", df['message'].iloc[0])
    print("APR√àS:", df['cleaned_message'].iloc[0])
    
    print("\nAVANT:", df['message'].iloc[100])
    print("APR√àS:", df['cleaned_message'].iloc[100])
    
    # S√©parer features et labels
    X = df['cleaned_message']
    y = df['label_num']
    
    # Split train/test (80% train, 20% test)
    print("\nüìä S√©paration train/test...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"‚úÖ Train set: {len(X_train)} messages")
    print(f"‚úÖ Test set: {len(X_test)} messages")
    
    # Vectorisation TF-IDF
    print("\nüî¢ Vectorisation TF-IDF...")
    X_train_tfidf = preprocessor.vectorize(X_train, fit=True)
    X_test_tfidf = preprocessor.vectorize(X_test, fit=False)
    
    print(f"‚úÖ Shape X_train: {X_train_tfidf.shape}")
    print(f"‚úÖ Shape X_test: {X_test_tfidf.shape}")
    
    # Cr√©er le dossier models s'il n'existe pas
    import os
    os.makedirs('models', exist_ok=True)
    
    # Sauvegarder les donn√©es pr√©trait√©es
    print("\nüíæ Sauvegarde des donn√©es...")
    with open('models/train_data.pkl', 'wb') as f:
        pickle.dump((X_train_tfidf, X_test_tfidf, y_train, y_test), f)
    
    # Sauvegarder le vectorizer
    preprocessor.save_vectorizer()
    
    print("\n‚úÖ Pr√©traitement termin√© avec succ√®s!")
    print("üìÅ Fichiers cr√©√©s:")
    print("   - models/train_data.pkl")
    print("   - models/vectorizer.pkl")


if __name__ == "__main__":
    main()